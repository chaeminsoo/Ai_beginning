{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmbcR0HxS7+w1M0G7lOXEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeminsoo/Ai_beginning/blob/master/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rTGKOLkKW9nj"
      },
      "outputs": [],
      "source": [
        "# 신경망의 일반적인 학습 과정은 다음과 같습니다\n",
        "\n",
        "# 학습 가능한 매개변수(또는 가중치(weight))를 갖는 신경망을 정의합니다.\n",
        "# 데이터셋(dataset) 입력을 반복합니다.\n",
        "# 입력을 신경망에서 전파(process)합니다.\n",
        "# 손실(loss; 출력이 정답으로부터 얼마나 떨어져있는지)을 계산합니다.\n",
        "# 변화도(gradient)를 신경망의 매개변수들에 역으로 전파합니다.\n",
        "# 신경망의 가중치를 갱신합니다. 일반적으로 다음과 같은 간단한 규칙을 사용합니다: 새로운 가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gradient)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "iGtSHsHei3o6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      # 입력 이비지 채널 1개, 출력 채널 6개, 5x5의 정사각 컨볼루션 행렬\n",
        "      # 컨볼루션 커널 정의\n",
        "      self.conv1 = nn.Conv2d(1,6,5)\n",
        "      self.conv2 = nn.Conv2d(6,16,5)\n",
        "      # 아핀(affine) 연산: y = Wx+b\n",
        "      self.fc1 = nn.Linear(16*5*5, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # (2,2) 크기의 윈도우에 대해 max pooling\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "      # 크기가 제곱 수라면, 하나의 숫자만을 특정(specify)\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "      x = torch.flatten(x,1) # 배치 차원을 제외한 모든 차원을 하나로 평탄화\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "EyI6nierjN0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4058cd4a-3b15-470d-b590-c802e41ff6bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward 정의하면 backward 자동 정의 (beacause of autograd)"
      ],
      "metadata": {
        "id": "9AUCfnwcBX4W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters()) # net.parameters(): 학습 가능한 매개 변수\n",
        "print(len(params))\n",
        "print(params[0].size()) # conv1의 .weight\n",
        "# print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLEx7ap7MGBQ",
        "outputId": "43eff562-f1b3-4c18-d07a-f2ad165e634b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = torch.randn(1,1,32,32)\n",
        "out = net(input_)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "hZMxvkUzL8Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90c9024-b887-4f19-b8d4-36fea21462ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1864, -0.0187, -0.0115, -0.1567, -0.0170, -0.0546,  0.0490,  0.0982,\n",
            "         -0.0334, -0.1262]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1,10))"
      ],
      "metadata": {
        "id": "zzSK2O_eSp0U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재까지 나온 것 요약\n",
        "\n",
        "# torch.Tensor: backward() 같은 autograd 연산을 지원하는 다차원 배열 입니다. 또한 tensor에 대한 변화도를 갖고 있습니다.\n",
        "# nn.Module: 신경망 모듈. 매개변수를 캡슐화(encapsulation)하는 간편한 방법 으로, GPU로 이동, 내보내기(exporting),\n",
        "#            불러오기(loading) 등의 작업을 위한 헬퍼(helper)를 제공합니다.\n",
        "# nn.Parameter: Tensor의 한 종류로, Module 에 속성으로 할당될 때 자동으로 매개변수로 등록 됩니다.\n",
        "# autograd.Function: autograd 연산의 순방향과 역방향 정의 를 구현합니다.\n",
        "#                    모든 Tensor 연산은 하나 이상의 Function 노드를 생성하며, 각 노드는 Tensor 를 생성하고 이력(history)을 인코딩 하는 함수들과 연결하고 있습니다.\n",
        "\n",
        "\n",
        "# 지금까지 다룬 것:\n",
        "# 신경망을 정의하는 것\n",
        "# 입력을 처리하고 backward를 호출하는 것\n",
        "\n",
        "# 앞으로의 내용:\n",
        "# 손실을 계산\n",
        "# 신경망 가중치를 갱신하는 것"
      ],
      "metadata": {
        "id": "L-Ap8GmJSqAB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수"
      ],
      "metadata": {
        "id": "Qw7f5rSASqJY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = net(input_)\n",
        "target = torch.rand(10) # 임의의 정답\n",
        "target = target.view(1,-1) # 출력과 같은 shape으로 만듦\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output,target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8k3W0Dzi6vO",
        "outputId": "61fc94c4-2ff3-4ceb-94bc-de4ea94fe3d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3745, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "#       -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "#       -> MSELoss\n",
        "#       -> loss"
      ],
      "metadata": {
        "id": "whrJxdz-i7RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N97UTJPoi7aV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}